{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìÑ SOW Generator (Google Colab Version)\n",
                "\n",
                "Generates a Statement of Work (SOW) PDF from Meeting Minutes using OpenAI.\n",
                "\n",
                "**Instructions:**\n",
                "1.  Add your API Key to Colab Secrets (`LLM_API_KEY`) or enter it when prompted.\n",
                "2.  Upload `SOW-TEMPLATE.pdf` to the Files section (sidebar).\n",
                "3.  Run the cells below step-by-step.\n",
                "\n",
                "> **Note on GPU:** This notebook is configured to use a GPU Runtime. However, since the current logic uses the OpenAI API (cloud-based), the local GPU will not significantly speed up the LLM generation. If you modify the code to use a local model (e.g., Llama-3 via Hugging Face), the GPU will be utilized."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Install Dependencies\n",
                "# @markdown Run this cell to install required libraries.\n",
                "import subprocess\n",
                "import sys\n",
                "\n",
                "def install(package):\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
                "\n",
                "print(\"Installing dependencies...\")\n",
                "try:\n",
                "    import openai\n",
                "    import pypdf\n",
                "    import xhtml2pdf\n",
                "except ImportError:\n",
                "    install(\"openai\")\n",
                "    install(\"pypdf\")\n",
                "    install(\"xhtml2pdf\")\n",
                "    print(\"Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Core Logic & Imports\n",
                "import os\n",
                "from io import BytesIO\n",
                "from openai import OpenAI\n",
                "from pypdf import PdfReader\n",
                "from xhtml2pdf import pisa\n",
                "import time\n",
                "\n",
                "# --- CORE FUNCTIONS (Ported from sow_generator.py) ---\n",
                "\n",
                "def extract_text_from_pdf(pdf_path):\n",
                "    \"\"\"Extracts text content from a PDF file.\"\"\"\n",
                "    try:\n",
                "        reader = PdfReader(pdf_path)\n",
                "        text = \"\"\n",
                "        for page in reader.pages:\n",
                "            text += page.extract_text() + \"\\n\"\n",
                "        return text\n",
                "    except Exception as e:\n",
                "        return f\"Error reading PDF template: {str(e)}\"\n",
                "\n",
                "def generate_pdf_from_html(html_content):\n",
                "    \"\"\"Converts HTML string to PDF bytes.\"\"\"\n",
                "    result = BytesIO()\n",
                "    pisa.CreatePDF(BytesIO(html_content.encode('utf-8')), result)\n",
                "    return result.getvalue()\n",
                "\n",
                "def chunk_text(text, size=2000):\n",
                "    return [text[i:i+size] for i in range(0, len(text), size)]\n",
                "\n",
                "def generate_sow_draft(mom_text, client, model):\n",
                "    \"\"\"Generates a text/markdown draft of the SOW from MOM details.\"\"\"\n",
                "    \n",
                "    # 1. Summarize MOM if too large\n",
                "    mom_chunks = chunk_text(mom_text)\n",
                "    consolidated_info = \"\"\n",
                "    \n",
                "    if len(mom_chunks) > 1:\n",
                "        print(f\"[+] Processing MOM in {len(mom_chunks)} batches...\")\n",
                "        for i, chunk in enumerate(mom_chunks, 1):\n",
                "            print(f\"    - Processing Batch {i}/{len(mom_chunks)}...\")\n",
                "            summary_prompt = (\n",
                "                f\"I have a section of Meeting Minutes. Extract key project details (Scope, Timeline, Budget, Team, Deliverables, etc.) as concise bullet points.\\n\"\n",
                "                f\"Ignore conversational filler.\\n\\n\"\n",
                "                f\"MOM Segment:\\n\"\n",
                "                f\"{chunk}\"\n",
                "            )\n",
                "            try:\n",
                "                resp = client.chat.completions.create(\n",
                "                    model=model,\n",
                "                    messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
                "                    temperature=0.3\n",
                "                )\n",
                "                consolidated_info += f\"\\n--- Batch {i} Details ---\\n{resp.choices[0].message.content}\\n\"\n",
                "            except Exception as e:\n",
                "                consolidated_info += f\"\\n[Error extracting Batch {i}: {str(e)}]\\n\"\n",
                "    else:\n",
                "        consolidated_info = mom_text\n",
                "\n",
                "    # 2. Draft Generation\n",
                "    print(\"[+] Generating SOW Draft...\")\n",
                "    system_prompt = \"You are a professional Project Manager and Technical Writer.\"\n",
                "    user_prompt = (\n",
                "        f\"I have the following Consolidated Project Details (extracted from meeting minutes):\\n\"\n",
                "        f\"---------------------\\n\"\n",
                "        f\"{consolidated_info}\\n\"\n",
                "        f\"---------------------\\n\\n\"\n",
                "        f\"Instructions:\\n\"\n",
                "        f\"1. Create a detailed Statement of Work (SOW) draft.\\n\"\n",
                "        f\"2. Use Markdown formatting (## Headers, - Bullet points).\\n\"\n",
                "        f\"3. Include standard SOW sections: Project Overview, Scope of Work, Deliverables, Timeline, Pricing/Budget, Governance/Team.\\n\"\n",
                "        f\"4. Do NOT use HTML tags yet. Just structure the content clearly.\\n\"\n",
                "    )\n",
                "    \n",
                "    try:\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt}\n",
                "            ],\n",
                "            temperature=0.3,\n",
                "        )\n",
                "        return response.choices[0].message.content\n",
                "    except Exception as e:\n",
                "        return f\"Error Generating SOW Draft: {str(e)}\"\n",
                "\n",
                "def format_sow_to_html(edited_text, template_text, client, model):\n",
                "    \"\"\"Wraps edited SOW text in HTML structure of the template.\"\"\"\n",
                "    print(\"[+] Applying Template Formatting...\")\n",
                "    \n",
                "    system_prompt = \"You are a specialized document formatter.\"\n",
                "    user_prompt = (\n",
                "        f\"I have the final SOW Content (Markdown):\\n\"\n",
                "        f\"---------------------\\n\"\n",
                "        f\"{edited_text}\\n\"\n",
                "        f\"---------------------\\n\\n\"\n",
                "        f\"And the Style/Structure extracted from a Reference PDF via OCR/Text Extraction:\\n\"\n",
                "        f\"---------------------\\n\"\n",
                "        f\"{template_text}\\n\"\n",
                "        f\"---------------------\\n\\n\"\n",
                "        f\"Instructions:\\n\"\n",
                "        f\"1. Convert the 'SOW Content' into an HTML document.\\n\"\n",
                "        f\"2. Mimic the structure and specific standard clauses found in the 'Reference PDF' where applicable, but keep the specific project details from 'SOW Content'.\\n\"\n",
                "        f\"3. Use HTML tags (<h1>, <p>, <ul> etc.).\\n\"\n",
                "        f\"4. Output ONLY the valid HTML. Do not include markdown code blocks.\\n\"\n",
                "    )\n",
                "    \n",
                "    try:\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": system_prompt},\n",
                "                {\"role\": \"user\", \"content\": user_prompt}\n",
                "            ],\n",
                "            temperature=0.1, \n",
                "        )\n",
                "        content = response.choices[0].message.content\n",
                "        content = content.replace(\"```html\", \"\").replace(\"```\", \"\").strip()\n",
                "        return content\n",
                "    except Exception as e:\n",
                "        return f\"<h3>Error Formatting SOW</h3><p>{str(e)}</p>\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Configuration & Setup\n",
                "LLM_BASE_URL = \"http://localhost:11434/engines/v1\" # @param {type:\"string\"}\n",
                "LLM_MODEL = \"llama3.2\" # @param {type:\"string\"}\n",
                "LLM_API_KEY= \"docker\"\n",
                "TEMPLATE_FILENAME = \"SOW-TEMPLATE.pdf\" # @param {type:\"string\"}\n",
                "\n",
                "# Retrieve API Key securely\n",
                "try:\n",
                "    from google.colab import userdata\n",
                "    LLM_API_KEY = userdata.get('LLM_API_KEY')\n",
                "except ImportError:\n",
                "    import getpass\n",
                "    print(\"Enter your LLM API Key:\")\n",
                "    LLM_API_KEY = getpass.getpass()\n",
                "except Exception:\n",
                "    import getpass\n",
                "    print(\"Could not retrieve key from userData. Enter your LLM API Key:\")\n",
                "    LLM_API_KEY = getpass.getpass()\n",
                "\n",
                "if not LLM_API_KEY:\n",
                "    raise ValueError(\"API Key is required!\")\n",
                "\n",
                "# Initialize Client\n",
                "client = OpenAI(base_url=LLM_BASE_URL, api_key=LLM_API_KEY)\n",
                "print(f\"‚úÖ Client initialized for {LLM_MODEL}\")\n",
                "\n",
                "# Check Template\n",
                "if not os.path.exists(TEMPLATE_FILENAME):\n",
                "    print(f\"‚ö†Ô∏è WARNING: '{TEMPLATE_FILENAME}' not found in the current directory.\")\n",
                "    print(\"Please upload the template PDF to the Files sidebar.\")\n",
                "    # Implement upload fallback if needed, or just warn\n",
                "    from google.colab import files\n",
                "    print(\"Upload template now?\")\n",
                "    uploaded = files.upload()\n",
                "    if TEMPLATE_FILENAME in uploaded:\n",
                "        print(f\"‚úÖ {TEMPLATE_FILENAME} uploaded.\")\n",
                "    else:\n",
                "        print(\"‚ùå Template upload failed or name mismatch.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Input Meeting Minutes\n",
                "# @markdown Run this cell and enter the MOM text when prompted.\n",
                "print(\"Paste your Meeting Minutes below and press Ctrl+D (Linux/Mac) or Ctrl+Z (Windows) followed by Enter to save:\")\n",
                "# Using standard input for multi-line paste in scripts, but in Colab input() is often better for short text. \n",
                "# For long text blocks in Colab, forms are better.\n",
                "MOM_TEXT = \"\"\"\n",
                "[PASTE YOUR MEETING MINUTES HERE IN THE CODE CELL IF PREFERRED OR USE INPUT BELOW]\n",
                "Project: New Web App\n",
                "Scope: Create a login page and dashboard.\n",
                "Timeline: 2 weeks.\n",
                "\"\"\" \n",
                "# In a real Colab cell, users can edit the string above directly.\n",
                "# Or use input()\n",
                "# MOM_TEXT = input(\"Paste MOM here (single line) or modify the MOM_TEXT variable in the code cell:\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 5. Generate SOW Draft\n",
                "if not MOM_TEXT or \"PASTE\" in MOM_TEXT:\n",
                "    print(\"‚ö†Ô∏è Please update the MOM_TEXT variable in standard text or provide input.\")\n",
                "else:\n",
                "    draft_sow = generate_sow_draft(MOM_TEXT, client, LLM_MODEL)\n",
                "    print(\"\\n\" + \"=\"*40)\n",
                "    print(\"GENERATED DRAFT SOW\")\n",
                "    print(\"=\"*40)\n",
                "    print(draft_sow)\n",
                "    print(\"=\"*40)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 6. Edit Draft (Optional)\n",
                "# @markdown Modify the `draft_sow` variable below if you want to make changes before generating the PDF.\n",
                "FINAL_DRAFT_SOW = draft_sow \n",
                "# In Colab, the user would interactively edit the cell logic or use a form. \n",
                "# We'll assume they proceed with FINAL_DRAFT_SOW."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 7. Generate PDF\n",
                "if os.path.exists(TEMPLATE_FILENAME):\n",
                "    template_content = extract_text_from_pdf(TEMPLATE_FILENAME)\n",
                "    html_out = format_sow_to_html(FINAL_DRAFT_SOW, template_content, client, LLM_MODEL)\n",
                "    pdf_bytes = generate_pdf_from_html(html_out)\n",
                "    \n",
                "    output_filename = \"Generated_SOW.pdf\"\n",
                "    with open(output_filename, \"wb\") as f:\n",
                "        f.write(pdf_bytes)\n",
                "    \n",
                "    print(f\"‚úÖ PDF Generated: {output_filename}\")\n",
                "    \n",
                "    from google.colab import files\n",
                "    files.download(output_filename)\n",
                "else:\n",
                "    print(\"‚ùå Template not found. Cannot generate formatted PDF.\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
