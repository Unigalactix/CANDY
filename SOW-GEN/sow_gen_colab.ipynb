{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìÑ SOW Generator (Local LLM / GPU Version)\n",
                "\n",
                "Generates a Statement of Work (SOW) PDF from Meeting Minutes using a **Local LLM** (Mistral-7B) running on Google Colab's GPU.\n",
                "\n",
                "**Instructions:**\n",
                "1.  **Runtime**: Ensure `Runtime > Change runtime type` is set to **T4 GPU** (or better).\n",
                "2.  **Hugging Face Token**: Add your HF Token to Colab Secrets (`HF_TOKEN`) with `READ` permission, or enter it when prompted. You must accept the model license for `mistralai/Mistral-7B-Instruct-v0.3` on Hugging Face.\n",
                "3.  **Template**: Upload `SOW-TEMPLATE.pdf` to the Files section.\n",
                "4.  Run all cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Install Dependencies\n",
                "# @markdown Installs `transformers`, `accelerate`, `bitsandbytes`, and PDF tools.\n",
                "import subprocess\n",
                "import sys\n",
                "\n",
                "def install(package):\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
                "\n",
                "print(\"Installing dependencies (this may take 1-2 mins)...\")\n",
                "try:\n",
                "    import torch\n",
                "    import transformers\n",
                "    import pypdf\n",
                "    import xhtml2pdf\n",
                "except ImportError:\n",
                "    install(\"torch\")\n",
                "    install(\"transformers\")\n",
                "    install(\"accelerate\")\n",
                "    install(\"bitsandbytes\")\n",
                "    install(\"pypdf\")\n",
                "    install(\"xhtml2pdf\")\n",
                "    print(\"Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Load Local LLM (Mistral-7B)\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
                "import os\n",
                "\n",
                "# CONFIGURATION\n",
                "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\" # @param {type:\"string\"}\n",
                "\n",
                "# Retrieve HF Token\n",
                "try:\n",
                "    from google.colab import userdata\n",
                "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
                "except ImportError:\n",
                "    import getpass\n",
                "    print(\"Enter your Hugging Face Token:\")\n",
                "    HF_TOKEN = getpass.getpass()\n",
                "except Exception:\n",
                "    import getpass\n",
                "    print(\"Could not retrieve key from userData. Enter your HF Token:\")\n",
                "    HF_TOKEN = getpass.getpass()\n",
                "\n",
                "if not HF_TOKEN:\n",
                "    raise ValueError(\"HF Token is required to download the model!\")\n",
                "\n",
                "print(f\"[+] Loading {MODEL_ID}... (This will take a few minutes)\")\n",
                "\n",
                "# Quantization Config (4-bit) to fit in T4 GPU\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n",
                "\n",
                "try:\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_ID,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\",\n",
                "        token=HF_TOKEN\n",
                "    )\n",
                "    \n",
                "    # Create Pipeline\n",
                "    pipe = pipeline(\n",
                "        \"text-generation\",\n",
                "        model=model,\n",
                "        tokenizer=tokenizer,\n",
                "        max_new_tokens=2500,\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "    print(\"‚úÖ Model loaded successfully on GPU!\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error loading model: {e}\")\n",
                "    print(\"Make sure you have accepted the model license on Hugging Face.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Core Functions (Adapted for Local LLM)\n",
                "from io import BytesIO\n",
                "from pypdf import PdfReader\n",
                "from xhtml2pdf import pisa\n",
                "\n",
                "def extract_text_from_pdf(pdf_path):\n",
                "    try:\n",
                "        reader = PdfReader(pdf_path)\n",
                "        text = \"\"\n",
                "        for page in reader.pages:\n",
                "            text += page.extract_text() + \"\\n\"\n",
                "        return text\n",
                "    except Exception as e:\n",
                "        return f\"Error reading PDF template: {str(e)}\"\n",
                "\n",
                "def generate_pdf_from_html(html_content):\n",
                "    result = BytesIO()\n",
                "    pisa.CreatePDF(BytesIO(html_content.encode('utf-8')), result)\n",
                "    return result.getvalue()\n",
                "\n",
                "def query_local_llm(messages, pipeline_obj):\n",
                "    \"\"\"Helper to query the local pipeline using Chat Templates.\"\"\"\n",
                "    prompt = pipeline_obj.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    outputs = pipeline_obj(\n",
                "        prompt, \n",
                "        do_sample=True, \n",
                "        temperature=0.3, \n",
                "        top_p=0.9\n",
                "    )\n",
                "    # Extract only the generated text (remove prompt)\n",
                "    generated_text = outputs[0][\"generated_text\"]\n",
                "    # Mistral/Llama usually appends response at the end. \n",
                "    # We need to robustly strip the input prompt if apply_chat_template doesn't handle it conceptually for the return.\n",
                "    # Usually pipeline returns properties differently. Let's slice:\n",
                "    return generated_text[len(prompt):].strip()\n",
                "\n",
                "def generate_sow_draft(mom_text, pipeline_obj):\n",
                "    print(\"[+] Generating SOW Draft (this may take time)...\")\n",
                "    \n",
                "    # Simplified One-Shot Prompt for Local LLM context window limits\n",
                "    messages = [\n",
                "        {\"role\": \"user\", \"content\": (\n",
                "            f\"You are a Project Manager. Create a detailed Statement of Work (SOW) based on these Meeting Minutes:\\n\\n\"\n",
                "            f\"'{mom_text}'\\n\\n\"\n",
                "            f\"Include sections: Project Overview, Scope, Deliverables, Timeline, Pricing.\\n\"\n",
                "            f\"Use Markdown formatting.\"\n",
                "        )}\n",
                "    ]\n",
                "    return query_local_llm(messages, pipeline_obj)\n",
                "\n",
                "def format_sow_to_html(edited_text, template_text, pipeline_obj):\n",
                "    print(\"[+] Formatting SOW to HTML...\")\n",
                "    \n",
                "    # We truncated template_text if it's too huge for local context\n",
                "    short_template = template_text[:1000] + \"...[truncated]\"\n",
                "    \n",
                "    messages = [\n",
                "        {\"role\": \"user\", \"content\": (\n",
                "            f\"Convert this SOW text into HTML. Mimic the style of the following template.\\n\\n\"\n",
                "            f\"TEMPLATE STYLE:\\n{short_template}\\n\\n\"\n",
                "            f\"SOW CONTENT:\\n{edited_text}\\n\\n\"\n",
                "            f\"Output ONLY valid HTML code. No markdown blocks.\"\n",
                "        )}\n",
                "    ]\n",
                "    response = query_local_llm(messages, pipeline_obj)\n",
                "    return response.replace(\"```html\", \"\").replace(\"```\", \"\").strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Input & Generate\n",
                "# @markdown Run this cell. Inputs MOM -> Generates Draft.\n",
                "\n",
                "MOM_TEXT = \"\"\"\n",
                "Project: Customer Portal Redesign\n",
                "Scope: Update UI/UX, add login, integrate Stripe payments.\n",
                "Timeline: 4 weeks. Budget: $10k.\n",
                "Team: 1 Designer, 2 Developers.\n",
                "\"\"\"\n",
                "# You can modify MOM_TEXT above or use input()\n",
                "\n",
                "if 'pipe' not in locals():\n",
                "    print(\"‚ùå Pipeline not loaded. Please run Cell 2 first.\")\n",
                "else:\n",
                "    draft_sow = generate_sow_draft(MOM_TEXT, pipe)\n",
                "    print(\"\\n\" + \"=\"*40)\n",
                "    print(\"GENERATED DRAFT SOW\")\n",
                "    print(\"=\"*40)\n",
                "    print(draft_sow)\n",
                "    print(\"=\"*40)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 5. Generate PDF\n",
                "# @markdown Generates the final PDF.\n",
                "\n",
                "FINAL_DRAFT_SOW = draft_sow # Modify this if you want to edit the text programmatically\n",
                "TEMPLATE_FILENAME = \"SOW-TEMPLATE.pdf\"\n",
                "\n",
                "if os.path.exists(TEMPLATE_FILENAME):\n",
                "    template_content = extract_text_from_pdf(TEMPLATE_FILENAME)\n",
                "    html_out = format_sow_to_html(FINAL_DRAFT_SOW, template_content, pipe)\n",
                "    pdf_bytes = generate_pdf_from_html(html_out)\n",
                "    \n",
                "    output_filename = \"Local_SOW.pdf\"\n",
                "    with open(output_filename, \"wb\") as f:\n",
                "        f.write(pdf_bytes)\n",
                "    \n",
                "    print(f\"‚úÖ PDF Generated: {output_filename}\")\n",
                "    from google.colab import files\n",
                "    files.download(output_filename)\n",
                "else:\n",
                "    print(\"‚ùå Template 'SOW-TEMPLATE.pdf' not found. Upload it and run again.\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}